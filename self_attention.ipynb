{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7751070f-b568-467d-ba35-f771684c9f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "ad4ec398-172b-489d-ad70-99c4775bf6ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_str = \"\"\"\n",
    "A transformer is a passive component that transfers electrical energy from one electrical circuit to another circuit, or multiple circuits. A varying current in any coil of the transformer produces a varying magnetic flux in the transformer's core, which induces a varying electromotive force (EMF) across any other coils wound around the same core. Electrical energy can be transferred between separate coils without a metallic (conductive) connection between the two circuits. Faraday's law of induction, discovered in 1831, describes the induced voltage effect in any coil due to a changing magnetic flux encircled by the coil.\n",
    "\n",
    "Transformers are used to change AC voltage levels, such transformers being termed step-up or step-down type to increase or decrease voltage level, respectively. Transformers can also be used to provide galvanic isolation between circuits as well as to couple stages of signal-processing circuits. Since the invention of the first constant-potential transformer in 1885, transformers have become essential for the transmission, distribution, and utilization of alternating current electric power.[1] A wide range of transformer designs is encountered in electronic and electric power applications. Transformers range in size from RF transformers less than a cubic centimeter in volume, to units weighing hundreds of tons used to interconnect the power grid.\n",
    "\"\"\"\n",
    "\n",
    "data = data_str.replace(\"\\n\", \"\").split(\" \")\n",
    "unique_data = list(set(data))\n",
    "n_embedding = len(unique_data)\n",
    "emb_dict = dict([(val, i) for i, val in enumerate(unique_data)])\n",
    "n_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69721c19-33d9-4044-ae65-c0703b717b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "a0860803-af59-4f53-96f3-84cd6f36f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionWithEmbedding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_init, n_out, n_embedding):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = np.sqrt(n_init)\n",
    "        \n",
    "        self.Embedding = torch.nn.Embedding(n_embedding, n_init)\n",
    "        \n",
    "        self.Q = torch.nn.Linear(n_init, n_init)\n",
    "        self.K = torch.nn.Linear(n_init, n_init)\n",
    "        self.V = torch.nn.Linear(n_init, n_init)\n",
    "        \n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        self.output = torch.nn.Linear(n_init**2, n_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_emb = self.Embedding(x)\n",
    "        \n",
    "        x_q = self.Q(x_emb)\n",
    "        x_k = self.K(x_emb)\n",
    "        x_v = self.V(x_emb)\n",
    "\n",
    "        x_k_t = torch.transpose(x_k, 1, 2)\n",
    "        \n",
    "        output = self.softmax(torch.matmul(x_q, x_k_t) / self.scale)\n",
    "        output = torch.matmul(output, x_v)\n",
    "        output = torch.flatten(output, start_dim=1)\n",
    "\n",
    "        output = self.output(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "9a5b4e1a-e6fb-46a1-b712-c509e7c0a8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of SelfAttentionWithEmbedding(\n",
       "  (Embedding): Embedding(131, 12)\n",
       "  (Q): Linear(in_features=12, out_features=12, bias=True)\n",
       "  (K): Linear(in_features=12, out_features=12, bias=True)\n",
       "  (V): Linear(in_features=12, out_features=12, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (output): Linear(in_features=144, out_features=131, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "1d1799ab-1e4b-4392-8f0a-3fdcfe33573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_emb = [emb_dict[val] for val in data[:12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "9c298b0d-d4c0-4c90-b44e-2f6130d6f590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[108, 28, 64, 17, 71, 114, 106, 23, 99, 12, 60, 78]"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "7dad1d56-01e9-4fef-b3f9-70970ef502f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'electrical'"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "bacf904d-26c4-4cf5-b493-1cebb5e8339b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dict[data[12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "9711ad21-c45f-47e4-a412-28fed05ccfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(data_emb).reshape(1, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "c4c3cc66-9c15-40f3-b63c-a714ec06a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "cff3c39b-4e08-4725-a122-ed15e8066ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = SelfAttentionWithEmbedding(12, 8, n_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "d4060e45-f56d-4fee-9ba2-2969abd75251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0248,  0.0173, -0.1710,  0.0514, -0.0041, -0.3186, -0.0025,  0.2035,\n",
       "         -0.0157,  0.0225,  0.0505, -0.1653,  0.0206, -0.1190,  0.0205, -0.1348,\n",
       "          0.1364, -0.2349,  0.2018, -0.2313,  0.0162, -0.0228,  0.1332,  0.0253,\n",
       "          0.0073,  0.1287, -0.2020,  0.0620, -0.0932,  0.0098,  0.2622,  0.0920,\n",
       "         -0.0019,  0.0501,  0.0100,  0.0077,  0.2352,  0.0547, -0.0805, -0.1764,\n",
       "          0.1908, -0.1180,  0.0113,  0.1625,  0.1603, -0.3160, -0.1270,  0.0599,\n",
       "         -0.0989,  0.1712,  0.1203, -0.1016,  0.0243, -0.0719, -0.0382,  0.0602,\n",
       "          0.1114,  0.0172,  0.0882, -0.1649, -0.3203,  0.0409,  0.1870, -0.0880,\n",
       "         -0.0185, -0.0700,  0.0333,  0.1328, -0.0137,  0.2448,  0.1801, -0.2372,\n",
       "         -0.1669,  0.1002, -0.0740,  0.0838,  0.0070,  0.0054, -0.0818,  0.1834,\n",
       "          0.1512, -0.1780, -0.0691, -0.0704,  0.0524,  0.1105, -0.5593,  0.4018,\n",
       "         -0.1180,  0.0725, -0.0204,  0.0799, -0.1308,  0.3303, -0.2414,  0.0246,\n",
       "          0.1403,  0.0641,  0.0220,  0.2855,  0.1905, -0.0152, -0.1219,  0.1319,\n",
       "         -0.0315, -0.1731, -0.2173,  0.3418, -0.3742, -0.0903, -0.2488, -0.2174,\n",
       "          0.0395, -0.1651,  0.2931, -0.1018,  0.4361,  0.0644,  0.0474, -0.2474,\n",
       "         -0.2271,  0.0331, -0.0993,  0.1983, -0.0659,  0.1240, -0.1459,  0.0082,\n",
       "          0.1856,  0.0146, -0.2882]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "394293de-eef7-4436-9579-7168a2a4ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, n_seq=12):\n",
    "\n",
    "    data_clear = data.replace(\"\\n\", \"\").split(\" \")\n",
    "    unique_data = list(set(data_clear))\n",
    "    n_embedding = len(unique_data)\n",
    "    emb_dict = dict([(val, i) for i, val in enumerate(unique_data)])\n",
    "    data_emb = [emb_dict[i] for i in data_clear]\n",
    "    \n",
    "    dataset_emb = np.array([[k for k in data_emb[i:(i+12)]] for i in range(len(data_emb) - n_seq - 1)])\n",
    "    y = [d[-1] for d in dataset_emb[1:]]\n",
    "    \n",
    "    x = torch.from_numpy(np.array(dataset_emb[:-1]))\n",
    "    y = torch.from_numpy(np.array(y, dtype='int64'))\n",
    "    y_onehot = torch.nn.functional.one_hot(y, num_classes=n_embedding).type(torch.DoubleTensor)\n",
    "    \n",
    "    \n",
    "    return torch.utils.data.TensorDataset(x, y_onehot)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "fa9dfd6d-58fb-492a-8b5a-135b9099629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset(data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "5b7089c4-5949-4b97-9893-2005803835e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[108,  28,  64,  17,  71, 114, 106,  23,  99,  12,  60,  78]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "ebe9cc0b-f58d-4822-baa8-9e914dbe56da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0248,  0.0173, -0.1710,  0.0514, -0.0041, -0.3186, -0.0025,  0.2035,\n",
       "         -0.0157,  0.0225,  0.0505, -0.1653,  0.0206, -0.1190,  0.0205, -0.1348,\n",
       "          0.1364, -0.2349,  0.2018, -0.2313,  0.0162, -0.0228,  0.1332,  0.0253,\n",
       "          0.0073,  0.1287, -0.2020,  0.0620, -0.0932,  0.0098,  0.2622,  0.0920,\n",
       "         -0.0019,  0.0501,  0.0100,  0.0077,  0.2352,  0.0547, -0.0805, -0.1764,\n",
       "          0.1908, -0.1180,  0.0113,  0.1625,  0.1603, -0.3160, -0.1270,  0.0599,\n",
       "         -0.0989,  0.1712,  0.1203, -0.1016,  0.0243, -0.0719, -0.0382,  0.0602,\n",
       "          0.1114,  0.0172,  0.0882, -0.1649, -0.3203,  0.0409,  0.1870, -0.0880,\n",
       "         -0.0185, -0.0700,  0.0333,  0.1328, -0.0137,  0.2448,  0.1801, -0.2372,\n",
       "         -0.1669,  0.1002, -0.0740,  0.0838,  0.0070,  0.0054, -0.0818,  0.1834,\n",
       "          0.1512, -0.1780, -0.0691, -0.0704,  0.0524,  0.1105, -0.5593,  0.4018,\n",
       "         -0.1180,  0.0725, -0.0204,  0.0799, -0.1308,  0.3303, -0.2414,  0.0246,\n",
       "          0.1403,  0.0641,  0.0220,  0.2855,  0.1905, -0.0152, -0.1219,  0.1319,\n",
       "         -0.0315, -0.1731, -0.2173,  0.3418, -0.3742, -0.0903, -0.2488, -0.2174,\n",
       "          0.0395, -0.1651,  0.2931, -0.1018,  0.4361,  0.0644,  0.0474, -0.2474,\n",
       "         -0.2271,  0.0331, -0.0993,  0.1983, -0.0659,  0.1240, -0.1459,  0.0082,\n",
       "          0.1856,  0.0146, -0.2882]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.nn.Sequential()\n",
    "model.add_module('fc1', layer)\n",
    "model(x[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "91d00664-f9ab-4170-aeb9-c03856314ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3069, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(186)\n",
      "tensor(0.3014, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(182)\n",
      "tensor(0.2977, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(180)\n",
      "tensor(0.2939, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(180)\n",
      "tensor(0.2902, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(180)\n",
      "tensor(0.2867, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(180)\n",
      "tensor(0.2831, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(179)\n",
      "tensor(0.2794, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(178)\n",
      "tensor(0.2754, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(178)\n",
      "tensor(0.2716, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(178)\n",
      "tensor(0.2671, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(176)\n",
      "tensor(0.2631, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(175)\n",
      "tensor(0.2584, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(175)\n",
      "tensor(0.2527, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(173)\n",
      "tensor(0.2463, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(172)\n",
      "tensor(0.2391, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(170)\n",
      "tensor(0.2304, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(168)\n",
      "tensor(0.2206, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(164)\n",
      "tensor(0.2101, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(156)\n",
      "tensor(0.1972, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(143)\n",
      "tensor(0.1837, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(126)\n",
      "tensor(0.1695, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(108)\n",
      "tensor(0.1543, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(90)\n",
      "tensor(0.1389, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(73)\n",
      "tensor(0.1238, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(57)\n",
      "tensor(0.1101, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(47)\n",
      "tensor(0.0971, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(38)\n",
      "tensor(0.0857, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(29)\n",
      "tensor(0.0750, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(24)\n",
      "tensor(0.0657, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(20)\n",
      "tensor(0.0579, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(17)\n",
      "tensor(0.0508, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(14)\n",
      "tensor(0.0447, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(10)\n",
      "tensor(0.0395, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(8)\n",
      "tensor(0.0349, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(6)\n",
      "tensor(0.0309, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(5)\n",
      "tensor(0.0275, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(3)\n",
      "tensor(0.0248, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(2)\n",
      "tensor(0.0220, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(1)\n",
      "tensor(0.0198, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0178, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0161, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0147, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0134, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0123, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0112, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0103, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0095, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0088, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0081, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0075, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0071, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0066, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0062, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0057, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0054, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0051, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0049, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0046, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0043, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0041, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0038, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0037, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0035, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0033, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0030, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0029, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0027, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0026, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0025, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0024, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0022, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0021, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0021, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0020, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0019, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0018, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0018, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0015, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0015, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(9.9869e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(9.8144e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(9.7318e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(9.6428e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(9.5279e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(9.4269e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(9.2812e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(9.2210e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(9.1164e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(9.0268e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(8.9045e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(8.8459e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(8.7425e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(8.6585e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(8.5648e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(8.4904e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(8.4217e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(8.3547e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(8.2710e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(8.2003e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(8.0348e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(7.9654e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(7.8953e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(7.8920e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(7.7285e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(7.7029e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(7.5620e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(7.5343e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(7.5251e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(7.3894e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(7.3013e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(7.2477e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(7.1357e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(7.0804e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(7.0093e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(6.9756e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(6.8891e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(6.8134e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(6.7168e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(6.6896e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(6.6343e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(6.5788e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(6.5455e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(6.4502e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(6.3703e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(6.2886e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(6.2315e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(6.1928e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(6.1260e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(6.0659e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(6.0254e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(5.9653e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n",
      "tensor(5.8963e-05, dtype=torch.float64, grad_fn=<DivBackward0>) errors: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential()\n",
    "model.add_module('fc1', layer)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "for t in range(300):\n",
    "    # Training loop for mini-batches\n",
    "    epoch_loss = 0\n",
    "    errors = 0\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        # Make predictions with the current parameters.\n",
    "        # x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        \n",
    "        # Compute the loss value.\n",
    "        # print(y_pred.shape, y.shape)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        # print(loss)\n",
    "        # input()\n",
    "        epoch_loss += torch.sum(loss) / 16\n",
    "        \n",
    "        # input()\n",
    "        errors += torch.sum(torch.argmax(y_pred, dim=1) != torch.argmax(y, dim=1))\n",
    "        \n",
    "        # Update the parameters.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(epoch_loss / (batch_idx + 1), 'errors:', errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "d5f35f1f-cac7-439b-b80f-e82d1f51be00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.max(\n",
       " values=tensor([13.8297, 20.3500, 15.4333, 16.2169, 14.9565, 15.1938, 18.7971, 16.5171,\n",
       "         13.4548, 12.8462, 17.9277, 12.8777, 14.1945], grad_fn=<MaxBackward0>),\n",
       " indices=tensor([  4, 111,  86,   1,  54, 102,  64,  84,  25,  42, 103,  67,  17])),\n",
       " tensor([  4, 111,  86,   1,  54, 102,  64,  84,  25,  42, 103,  67,  17]),\n",
       " tensor([  4, 111,  86,   1,  54, 102,  64,  84,  25,  42, 103,  67,  17]))"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(model(x), dim=1), torch.argmax(model(x), dim=1), torch.argmax(y, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff0d935-b086-433b-8cca-40b93c88861d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "4641e986-97e8-4442-a112-42af6d5ce583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad689d33-c6e4-4c56-aa4f-a00cd5070f53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
